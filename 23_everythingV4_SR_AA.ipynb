{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b",
   "metadata": {
    "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b"
   },
   "source": [
    "## 라이브러리 호출 및 I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a",
   "metadata": {
    "executionInfo": {
     "elapsed": 5126,
     "status": "ok",
     "timestamp": 1630047280444,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a"
   },
   "outputs": [],
   "source": [
    "import os, torch, copy, cv2, sys, random, logging\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from adamp import AdamP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe733f8-2e09-4197-8fa8-3d166c7b5101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1670048"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd8a1e9-a121-4fd3-9af4-23f1bbc2d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9c4250-2257-404f-941d-58eff1e9eb38",
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1630047303454,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "8f9c4250-2257-404f-941d-58eff1e9eb38"
   },
   "outputs": [],
   "source": [
    "# # 시드(seed) 설정\n",
    "\n",
    "# RANDOM_SEED = 2021\n",
    "# torch.manual_seed(RANDOM_SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# np.random.seed(RANDOM_SEED)\n",
    "# random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e573764-7fdc-4e57-9a0a-39833df1d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed = 100\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6AYoEZyP1V_V",
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1630047300749,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "6AYoEZyP1V_V"
   },
   "outputs": [],
   "source": [
    "def get_logger(name: str, file_path: str, stream=False) -> logging.RootLogger:\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s')\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler(file_path)\n",
    "\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    if stream:\n",
    "        logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954",
   "metadata": {
    "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954"
   },
   "source": [
    "## Argument Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9",
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1630047307151,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9"
   },
   "outputs": [],
   "source": [
    "# working directory 지정\n",
    "ROOT_PATH = './'\n",
    "TRAIN_DIR = os.path.join(ROOT_PATH, 'train')\n",
    "RESULT_DIR = os.path.join(ROOT_PATH, 'results')\n",
    "WEIGHT_DIR = os.path.join(ROOT_PATH, 'weights')\n",
    "NUMPY_DIR = os.path.join(ROOT_PATH, 'numpy')\n",
    "CSV_DIR = os.path.join(ROOT_PATH, 'csv')\n",
    "TEST_DIR = os.path.join(ROOT_PATH, 'test')\n",
    "TEST_ANNOT_DIR = os.path.join(ROOT_PATH, 'test_annot')\n",
    "\n",
    "if not os.path.isdir(RESULT_DIR):\n",
    "    os.makedirs(RESULT_DIR)\n",
    "\n",
    "# hyper-parameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 1e-8\n",
    "EARLY_STOPPING_PATIENCE = 6\n",
    "INPUT_SHAPE = (480, 240)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44807b0-7788-49ec-aff2-c756e4513c5e",
   "metadata": {
    "id": "d44807b0-7788-49ec-aff2-c756e4513c5e"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05",
   "metadata": {
    "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05"
   },
   "source": [
    "#### Train & Validation Set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04642777-c2e0-439b-9692-f6c571a86521",
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1630047309743,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "04642777-c2e0-439b-9692-f6c571a86521"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, db, mode):\n",
    "\n",
    "        self.db = db\n",
    "        self.mode = mode\n",
    "        self.transform = transforms.Compose([\n",
    "                                transforms.Resize(INPUT_SHAPE),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5)])\n",
    "        \n",
    "        self.autoaugment = transforms.Compose([\n",
    "            transforms.RandomApply(nn.ModuleList([\n",
    "                transforms.AutoAugment()]), p=0.5),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "        trans_image = TF.crop(trans_image, 0,0,INPUT_SHAPE[1], INPUT_SHAPE[1])\n",
    "        trans_image = self.autoaugment(trans_image)\n",
    "        \n",
    "        return trans_image, data['label']\n",
    "\n",
    "    \n",
    "def data_loader(data_dir=TRAIN_DIR):\n",
    "    print('Loading ' + ' dataset..')\n",
    "    if not os.path.isdir(data_dir):\n",
    "        print(f'!!! Cannot find {data_dir}... !!!')\n",
    "        sys.exit()\n",
    "        \n",
    "    mask_image_list = os.listdir(os.path.join(data_dir, 'Mask'))\n",
    "    nomask_image_list = os.listdir(os.path.join(data_dir, 'NoMask'))\n",
    "    mask_image_list = [item for item in mask_image_list if item[-4:] == '.png']\n",
    "    nomask_image_list = [item for item in nomask_image_list  if item[-4:] == '.png']\n",
    "    mask_image_path = list(map(lambda x : os.path.join(data_dir, 'Mask', x), mask_image_list))\n",
    "    nomask_image_path = list(map(lambda x : os.path.join(data_dir, 'NoMask', x), nomask_image_list))\n",
    "\n",
    "    # encoding label (Mask : 1, No Mask : 0)\n",
    "    mask_df = pd.DataFrame({'img_path':mask_image_path, 'label':np.ones(len(mask_image_list))})\n",
    "    nomask_df = pd.DataFrame({'img_path':nomask_image_path, 'label':np.zeros(len(nomask_image_list))})\n",
    "    db = mask_df.append(nomask_df, ignore_index=True)\n",
    "    return db\n",
    "\n",
    "\n",
    "if os.path.isfile(os.path.join(TRAIN_DIR, 'total.pkl')):\n",
    "    db = pd.read_pickle(os.path.join(TRAIN_DIR, 'total.pkl'))\n",
    "else:\n",
    "    db = data_loader()\n",
    "    db.to_pickle(os.path.join(TRAIN_DIR, 'total.pkl'))\n",
    "    \n",
    "    \n",
    "# Do stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_idx, valid_idx= train_test_split(\n",
    "    np.arange(len(db)),\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    stratify=db.label.values,\n",
    "    random_state=seed)\n",
    "\n",
    "train_db = db.iloc[train_idx]\n",
    "valid_db = db.iloc[valid_idx]\n",
    "\n",
    "train_db = train_db.reset_index()\n",
    "valid_db = valid_db.reset_index()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(INPUT_SHAPE),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07040416-e5dc-41f6-a567-7a779dd4ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set samples: 24388 Val set samples: 2710\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(db=train_db, mode='train')\n",
    "validation_dataset = CustomDataset(db=valid_db, mode='val')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print('Train set samples:',len(train_dataset),  'Val set samples:', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b27520-c82c-4ec8-ae0b-119a79167f09",
   "metadata": {
    "id": "61b27520-c82c-4ec8-ae0b-119a79167f09"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d34769e-afab-46ef-9168-94189235e48d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5400,
     "status": "ok",
     "timestamp": 1630047317233,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "2d34769e-afab-46ef-9168-94189235e48d",
    "outputId": "c6b7da67-22cf-4835-b4ba-0dce62980eca"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/rwightman/pytorch-image-models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c37334-3edd-4687-a88e-f5948bc15ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7f96e9d-dfed-412d-9dd8-02b99970b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a722d66-0cc7-4e49-9cdc-b90628407b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFF_BACKBONES = [\n",
    "    'tf_efficientnetv2_m_in21k'\n",
    "]\n",
    "NFN_BACKBONES = [\n",
    "    'eca_nfnet_l1',\n",
    "    'dm_nfnet_f2'    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84ff9566-36e8-4a7d-8e23-af605a8ec3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebuggerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DebuggerNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 3, 3, 3)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(2)\n",
    "        self.linear = nn.Linear(12, 2)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        x = self.avgpool(input_img)\n",
    "\n",
    "        x = x.view(-1, 12)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bef3b15-f18b-4aa9-b6a0-65822041b4e9",
   "metadata": {
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1630047325374,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "3bef3b15-f18b-4aa9-b6a0-65822041b4e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "# for efficientnet\n",
    "class EFFMaskClassifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(EFFMaskClassifier, self).__init__()\n",
    "        self.model = timm.create_model(backbone, pretrained=True)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, 2)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        x = self.model(input_img)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee11cfc8-9b26-4250-950a-3e02053009f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nfnet\n",
    "class NFNMaskClassifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(NFNMaskClassifier, self).__init__()\n",
    "        self.model = timm.create_model(backbone, pretrained=True)\n",
    "        n_features = self.model.head.fc.in_features\n",
    "        self.model.head.fc = nn.Linear(n_features, 2)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        x = self.model(input_img)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9fb549-609a-43e0-be73-95d9c9729713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for deit\n",
    "class DEITMaskClassifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(DEITMaskClassifier, self).__init__()\n",
    "        self.model = timm.create_model(backbone, pretrained=True)\n",
    "        in_features_head = self.model.head.in_features\n",
    "        in_features_head_dist = self.model.head_dist.in_features\n",
    "        \n",
    "        self.model.head = nn.Linear(in_features_head, 2)\n",
    "        self.model.head_dist = nn.Linear(in_features_head_dist, 2)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        x = self.model(input_img)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba438c6-44d1-4960-be0c-570d97bbb7e3",
   "metadata": {},
   "source": [
    "## Just Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3345b82-115e-4130-b53b-706d30c20dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion, model, device, metric_fn, optimizer=None, scheduler=None, logger=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            loss = self.criterion(pred, label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            pred = F.softmax(pred)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            train_total_loss += loss.item()\n",
    "            \n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "        \n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            pred = F.softmax(pred)\n",
    "            ## coordinate loss\n",
    "            loss = self.criterion(pred, label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, ROC: {auroc}'\n",
    "        print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a5722-df20-4d90-9be8-f1c98fa31009",
   "metadata": {},
   "source": [
    "## SAM Optimizer w. AdamP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fbbd9cf-af12-46ab-9365-39e775e22781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaffd8d-b025-42c1-8dd8-69529487389e",
   "metadata": {
    "id": "1aaffd8d-b025-42c1-8dd8-69529487389e"
   },
   "source": [
    "## MultiScale Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5faaac1b-64c3-4659-82de-d4309502f29a",
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1630047330626,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "5faaac1b-64c3-4659-82de-d4309502f29a"
   },
   "outputs": [],
   "source": [
    "class MultiScale_Trainer():\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion, model, device, metric_fn, scales, optimizer=None, scheduler=None, logger=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.scales = scales\n",
    "        self.scales_max_index = len(self.scales) - 1\n",
    "        self.criterion = criterion\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            \n",
    "            img = TF.resize(img, self.scales[np.random.randint(0, self.scales_max_index)])\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            loss = self.criterion(pred, label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            ## coordinate loss\n",
    "            loss = self.criterion(pred, label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8397f3c-bb85-4bf5-928c-ed4ce82ae232",
   "metadata": {},
   "source": [
    "# SAM Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ebf28b9-1118-417e-8122-02f6bfd33624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM_Trainer():\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion, model, device, metric_fn, optimizer=None, scheduler=None, logger=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "        \n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            loss = self.criterion(pred, label)\n",
    "            loss.backward()\n",
    "            self.optimizer.first_step(zero_grad=True)\n",
    "            self.criterion(self.model(img), label).backward()\n",
    "            self.optimizer.second_step(zero_grad=True)\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            ## coordinate loss\n",
    "            loss = self.criterion(pred, label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aca506-d168-4c9f-8eca-5cdecb122961",
   "metadata": {
    "id": "e2aca506-d168-4c9f-8eca-5cdecb122961"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33678d90-a254-48d5-bf09-2a817eeafea3",
   "metadata": {
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1630047333382,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "33678d90-a254-48d5-bf09-2a817eeafea3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def get_metric_fn(y_pred, y_answer, y_prob):\n",
    "    \"\"\" 성능을 반환하는 함수\n",
    "    \"\"\"\n",
    "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
    "    accuracy = accuracy_score(y_answer, y_pred)\n",
    "    auroc = roc_auc_score(y_answer, y_prob)\n",
    "    return accuracy, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c20b881c-efdd-4512-bf48-bb36a6c34672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaylorSoftmax(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=1, n=2):\n",
    "        super(TaylorSoftmax, self).__init__()\n",
    "        assert n % 2 == 0\n",
    "        self.dim = dim\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        fn = torch.ones_like(x)\n",
    "        denor = 1.\n",
    "        for i in range(1, self.n+1):\n",
    "            denor *= i\n",
    "            fn = fn + x.pow(i) / denor\n",
    "        out = fn / fn.sum(dim=self.dim, keepdims=True)\n",
    "        return out\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1): \n",
    "        super(LabelSmoothingLoss, self).__init__() \n",
    "        self.confidence = 1.0 - smoothing \n",
    "        self.smoothing = smoothing \n",
    "        self.cls = classes \n",
    "        self.dim = dim \n",
    "    def forward(self, pred, target): \n",
    "        \"\"\"Taylor Softmax and log are already applied on the logits\"\"\"\n",
    "        #pred = pred.log_softmax(dim=self.dim) \n",
    "        with torch.no_grad(): \n",
    "            true_dist = torch.zeros_like(pred) \n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1)) \n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "    \n",
    "class TaylorCrossEntropyLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, n=2, ignore_index=-1, reduction='mean', smoothing=0.1):\n",
    "        super(TaylorCrossEntropyLoss, self).__init__()\n",
    "        assert n % 2 == 0\n",
    "        self.taylor_softmax = TaylorSoftmax(dim=1, n=n)\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "        self.lab_smooth = LabelSmoothingLoss(2, smoothing=smoothing)\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "\n",
    "        log_probs = self.taylor_softmax(logits).log()\n",
    "        #loss = F.nll_loss(log_probs, labels, reduction=self.reduction,\n",
    "        #        ignore_index=self.ignore_index)\n",
    "        loss = self.lab_smooth(log_probs, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331",
   "metadata": {
    "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331"
   },
   "source": [
    "#### Test set Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a60a4c6-306e-499d-b071-2ab39b795b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ANNOT_DIR = os.path.join(ROOT_PATH, 'test_annot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a52193e8-3033-4d4b-9d00-754be781170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckDataset(Dataset):\n",
    "    def __init__(self, db, mode, transform):\n",
    "\n",
    "        self.db = db\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff2e6cb0-159f-4a25-bb72-779a09b4ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_loader(data_dir=TEST_ANNOT_DIR):\n",
    "    print('Loading ' + ' dataset..')\n",
    "    if not os.path.isdir(data_dir):\n",
    "        print(f'!!! Cannot find {data_dir}... !!!')\n",
    "        sys.exit()\n",
    "        \n",
    "    mask_image_list = os.listdir(os.path.join(data_dir, 'Mask'))\n",
    "    nomask_image_list = os.listdir(os.path.join(data_dir, 'NoMask'))\n",
    "    mask_image_list = [item for item in mask_image_list if item[-4:] == '.png']\n",
    "    nomask_image_list = [item for item in nomask_image_list  if item[-4:] == '.png']\n",
    "    mask_image_path = list(map(lambda x : os.path.join(data_dir, 'Mask', x), mask_image_list))\n",
    "    nomask_image_path = list(map(lambda x : os.path.join(data_dir, 'NoMask', x), nomask_image_list))\n",
    "\n",
    "    # encoding label (Mask : 1, No Mask : 0)\n",
    "    mask_df = pd.DataFrame({'img_path':mask_image_path, 'label':np.ones(len(mask_image_list))})\n",
    "    nomask_df = pd.DataFrame({'img_path':nomask_image_path, 'label':np.zeros(len(nomask_image_list))})\n",
    "    db = mask_df.append(nomask_df, ignore_index=True)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30a3f76d-abfe-4a26-a15a-8626de7221ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(os.path.join(TEST_ANNOT_DIR, 'test.pkl')):\n",
    "    test_db = pd.read_pickle(os.path.join(TEST_ANNOT_DIR, 'test.pkl'))\n",
    "else:\n",
    "    test_db = check_loader()\n",
    "    test_db.to_pickle(os.path.join(TEST_ANNOT_DIR, 'test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79d51d04-ea62-466d-b988-4e782c9aa5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading test dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        image_list = os.listdir(self.data_dir)\n",
    "        image_list = [item for item in image_list if item[-4:] == '.png']\n",
    "        image_path = list(map(lambda x : os.path.join(self.data_dir, x), image_list))\n",
    "        db = pd.DataFrame({'img_path':image_path, 'file_num':list(map(lambda x : x.split('.')[0], image_list))})\n",
    "        return db\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "        \n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['file_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "078c24da-fa47-4b4f-a36c-db1c961143b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(INPUT_SHAPE),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6127,
     "status": "ok",
     "timestamp": 1630047496452,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
    "outputId": "191d15b8-07ab-4d56-a2af-071ec0ec4b8c"
   },
   "outputs": [],
   "source": [
    "check_dataset = CheckDataset(db=test_db, mode='check', transform=transform)\n",
    "check_dataloader = DataLoader(check_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ac79993-ec06-425d-8934-dad2f1d2ae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset..\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(data_dir=TEST_DIR, input_shape=INPUT_SHAPE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d796ac-7b07-44ac-b453-40699fbc1336",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e60e2016-ef06-4ae9-8ac1-948347c4b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss_history, valid_loss_history, backbone):\n",
    "    plt.plot(train_loss_history, label=\"train loss\")\n",
    "    plt.plot(valid_loss_history, label=\"valid loss\")\n",
    "    plt.legend()\n",
    "    plt.title(backbone)\n",
    "    plt.show()\n",
    "\n",
    "def plot_acc(train_acc_history, valid_acc_history, backbone):\n",
    "    plt.plot(train_acc_history, label=\"train acc\")\n",
    "    plt.plot(valid_acc_history, label=\"valid acc\")\n",
    "    plt.legend()\n",
    "    plt.title(backbone)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c41f1889-f394-4226-b535-5c9e554db26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_and_flush():\n",
    "    \n",
    "    print(\"waiting...\")\n",
    "    \n",
    "    time.sleep(120)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"flushing...\")\n",
    "    time.sleep(60)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5fcfeb6-c198-4906-90c5-5254bbb36bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_size(size):\n",
    "\n",
    "    \"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "    assert(isinstance(size, torch.Size))\n",
    "    return \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "    \"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "    import gc\n",
    "    total_size = 0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "                                          \" GPU\" if obj.is_cuda else \"\",\n",
    "                                          \" pinned\" if obj.is_pinned else \"\",\n",
    "                                          pretty_size(obj.size())))\n",
    "                    total_size += obj.numel()\n",
    "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "                                                   type(obj.data).__name__, \n",
    "                                                   \" GPU\" if obj.is_cuda else \"\",\n",
    "                                                   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "                                                   \" grad\" if obj.requires_grad else \"\", \n",
    "                                                   \" volatile\" if obj.volatile else \"\",\n",
    "                                                   pretty_size(obj.data.size())))\n",
    "                    total_size += obj.data.numel()\n",
    "        except Exception as e:\n",
    "            pass        \n",
    "    print(\"Total size:\", total_size)\n",
    "\n",
    "# dump_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00409945-7c70-4633-8923-1e48d17053de",
   "metadata": {},
   "source": [
    "# 학습 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9f17d1e-e9ee-4b9f-ab39-6d6b15073c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EFF_train_val_infer(BACKBONE):\n",
    "    model = EFFMaskClassifier(BACKBONE).to(device)\n",
    "\n",
    "    base_optimizer = AdamP\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler =  optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=5, T_mult=2, eta_min=1e-5, last_epoch=-1)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric_fn = get_metric_fn\n",
    "\n",
    "    # Set system logger\n",
    "    system_logger = get_logger(name='train',file_path='train_log.log')\n",
    "\n",
    "    trainer = SAM_Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)\n",
    "    # trainer = MultiScale_Trainer(criterion, model, device, metric_fn, scales, optimizer, scheduler, logger=system_logger)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    valid_loss_history = []\n",
    "    valid_acc_history = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_acc = None\n",
    "    \n",
    "    patience = 0\n",
    "    \n",
    "    for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "        trainer.train_epoch(train_dataloader, epoch_index)\n",
    "        trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "        train_loss_history.append(trainer.train_mean_loss)\n",
    "        train_acc_history.append(trainer.train_score)\n",
    "\n",
    "        valid_loss_history.append(trainer.val_mean_loss)\n",
    "        valid_acc_history.append(trainer.validation_score)\n",
    "\n",
    "        # don't use early stopper\n",
    "        patience += 1\n",
    "        \n",
    "        if best_val_loss == None or trainer.val_mean_loss < best_val_loss:\n",
    "            best_val_loss = trainer.val_mean_loss\n",
    "            patience = 0\n",
    "            check_point = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(check_point, os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_AA_best.pt'))\n",
    "        \n",
    "        if patience == EARLY_STOPPING_PATIENCE:\n",
    "            break\n",
    "            \n",
    "    best_val_acc = max(valid_acc_history)\n",
    "\n",
    "    plot_loss(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    plot_acc(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    \n",
    "    TRAINED_MODEL_PATH = os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_AA_best.pt')\n",
    "    \n",
    "    model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "    # Prediction\n",
    "    file_num_lst = []\n",
    "    pred_lst = []\n",
    "    prob_lst = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (img, file_num) in enumerate(test_dataloader):\n",
    "            img = img.to(device)\n",
    "            pred = model(img)\n",
    "            pred = F.softmax(pred)\n",
    "            file_num_lst.extend(list(file_num))\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'file_name':list(map(int,file_num_lst)), 'answer':pred_lst, 'prob':prob_lst})\n",
    "    df.sort_values(by=['file_name'], inplace=True)\n",
    "    df.to_csv(os.path.join(RESULT_DIR, f'mask_pred_with_{BACKBONE}_SAM_AdamP_AA.csv'), index=False)\n",
    "\n",
    "    trainer.validate_epoch(check_dataloader, 0)\n",
    "    \n",
    "    # save data\n",
    "    \n",
    "    TIME = datetime.now().strftime('%m%d%H%M%S')\n",
    "\n",
    "    save_dict = {'backbone':BACKBONE,\n",
    "                 'time':TIME,\n",
    "                 'best_val_loss':best_val_loss,\n",
    "                 'best_val_acc':best_val_acc,\n",
    "                 'test_acc':None,\n",
    "                 'epochs':EPOCHS,\n",
    "                 'input_shape':INPUT_SHAPE,\n",
    "                 'augmentation':None,\n",
    "                 'optimizer':'SAM, AdamP',\n",
    "                 'scheduler':'CosineAnnealingWarmRestarts',\n",
    "                 'attention_module':None,\n",
    "                 'learning_rate':LEARNING_RATE,\n",
    "                 'batch_size':BATCH_SIZE,\n",
    "                 'loss':'CrossEntropyLoss',\n",
    "                 'freeze':None,\n",
    "                 'others':None,\n",
    "                 'randomeseed':seed,\n",
    "                 'train_loss':train_loss_history,\n",
    "                 'train_acc':train_acc_history,\n",
    "                 'valid_loss':valid_loss_history,\n",
    "                 'valid_acc':valid_acc_history}\n",
    "\n",
    "\n",
    "    with open(f\"{os.path.join(NUMPY_DIR, BACKBONE)}_SAM_AdamP_AA_{TIME}.json\", 'w') as f:\n",
    "        json.dump(save_dict, f)\n",
    "\n",
    "    wait_and_flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf39f4e9-6b00-4bc6-ab84-85863b98ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NFN_train_val_infer(BACKBONE):\n",
    "    model = NFNMaskClassifier(BACKBONE).to(device)\n",
    "\n",
    "    base_optimizer = AdamP\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler =  optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=5, T_mult=2, eta_min=1e-5, last_epoch=-1)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric_fn = get_metric_fn\n",
    "\n",
    "    # Set system logger\n",
    "    system_logger = get_logger(name='train',file_path='train_log.log')\n",
    "\n",
    "    trainer = SAM_Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)\n",
    "    # trainer = MultiScale_Trainer(criterion, model, device, metric_fn, scales, optimizer, scheduler, logger=system_logger)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    valid_loss_history = []\n",
    "    valid_acc_history = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_acc = None\n",
    "    \n",
    "    patience = 0\n",
    "    \n",
    "    for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "        trainer.train_epoch(train_dataloader, epoch_index)\n",
    "        trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "        train_loss_history.append(trainer.train_mean_loss)\n",
    "        train_acc_history.append(trainer.train_score)\n",
    "\n",
    "        valid_loss_history.append(trainer.val_mean_loss)\n",
    "        valid_acc_history.append(trainer.validation_score)\n",
    "\n",
    "        # don't use early stopper\n",
    "        patience += 1\n",
    "        \n",
    "        if best_val_loss == None or trainer.val_mean_loss < best_val_loss:\n",
    "            best_val_loss = trainer.val_mean_loss\n",
    "            patience = 0\n",
    "            check_point = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(check_point, os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_AA_best.pt'))\n",
    "        \n",
    "        if patience == EARLY_STOPPING_PATIENCE:\n",
    "            break\n",
    "            \n",
    "    best_val_acc = max(valid_acc_history)\n",
    "\n",
    "    plot_loss(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    plot_acc(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    \n",
    "    TRAINED_MODEL_PATH = os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_AA_best.pt')\n",
    "    \n",
    "    model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "    # Prediction\n",
    "    file_num_lst = []\n",
    "    pred_lst = []\n",
    "    prob_lst = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (img, file_num) in enumerate(test_dataloader):\n",
    "            img = img.to(device)\n",
    "            pred = model(img)\n",
    "            pred = F.softmax(pred)\n",
    "            file_num_lst.extend(list(file_num))\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'file_name':list(map(int,file_num_lst)), 'answer':pred_lst, 'prob':prob_lst})\n",
    "    df.sort_values(by=['file_name'], inplace=True)\n",
    "    df.to_csv(os.path.join(RESULT_DIR, f'mask_pred_with_{BACKBONE}_SAM_AdamP_AA.csv'), index=False)\n",
    "\n",
    "    trainer.validate_epoch(check_dataloader, 0)\n",
    "    \n",
    "    # save data\n",
    "    \n",
    "    TIME = datetime.now().strftime('%m%d%H%M%S')\n",
    "\n",
    "    save_dict = {'backbone':BACKBONE,\n",
    "                 'time':TIME,\n",
    "                 'best_val_loss':best_val_loss,\n",
    "                 'best_val_acc':best_val_acc,\n",
    "                 'test_acc':None,\n",
    "                 'epochs':EPOCHS,\n",
    "                 'input_shape':INPUT_SHAPE,\n",
    "                 'augmentation':None,\n",
    "                 'optimizer':'SAM, AdamP',\n",
    "                 'scheduler':'CosineAnnealingWarmRestarts',\n",
    "                 'attention_module':None,\n",
    "                 'learning_rate':LEARNING_RATE,\n",
    "                 'batch_size':BATCH_SIZE,\n",
    "                 'loss':'CrossEntropyLoss',\n",
    "                 'freeze':None,\n",
    "                 'others':None,\n",
    "                 'randomeseed':seed,\n",
    "                 'train_loss':train_loss_history,\n",
    "                 'train_acc':train_acc_history,\n",
    "                 'valid_loss':valid_loss_history,\n",
    "                 'valid_acc':valid_acc_history}\n",
    "\n",
    "\n",
    "    with open(f\"{os.path.join(NUMPY_DIR, BACKBONE)}_SAM_AdamP_AA_{TIME}.json\", 'w') as f:\n",
    "        json.dump(save_dict, f)\n",
    "\n",
    "    wait_and_flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ad3aa24-d241-469d-b623-f79450ae0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEIT_train_val_infer(BACKBONE):\n",
    "    model = DEITMaskClassifier(BACKBONE).to(device)\n",
    "\n",
    "    # For Multi-GPU\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1'\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "    # Set optimizer, scheduler, loss function, metric function\n",
    "    base_optimizer = AdamP\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler =  optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=6, T_mult=1, eta_min=1e-6)\n",
    "    # criterion = TaylorCrossEntropyLoss(n=2, smoothing=0.1)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric_fn = get_metric_fn\n",
    "\n",
    "    # Set system logger\n",
    "    system_logger = get_logger(name='train',file_path='train_log.log')\n",
    "\n",
    "    # Set trainer\n",
    "    scales = [\n",
    "        [140, 70], [180, 90], [220, 110], \n",
    "        [260, 130], [300, 150], [340, 170],\n",
    "        [380, 190], [420, 210], [460, 230]\n",
    "             ]\n",
    "\n",
    "    trainer = SAM_Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)\n",
    "    # trainer = MultiScale_Trainer(criterion, model, device, metric_fn, scales, optimizer, scheduler, logger=system_logger)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    valid_loss_history = []\n",
    "    valid_acc_history = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_acc = None\n",
    "\n",
    "    \n",
    "    for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "        trainer.train_epoch(train_dataloader, epoch_index)\n",
    "        trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "        train_loss_history.append(trainer.train_mean_loss)\n",
    "        train_acc_history.append(trainer.train_score)\n",
    "\n",
    "        valid_loss_history.append(trainer.val_mean_loss)\n",
    "        valid_acc_history.append(trainer.validation_score)\n",
    "\n",
    "        # don't use early stopper\n",
    "\n",
    "        if best_val_loss == None or trainer.val_mean_loss < best_val_loss:\n",
    "            best_val_loss = trainer.val_mean_loss\n",
    "            criterion = trainer.val_mean_loss\n",
    "            check_point = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(check_point, os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_v2_best.pt'))\n",
    "\n",
    "    best_val_acc = max(valid_acc_history)\n",
    "\n",
    "    plot_loss(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    plot_acc(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    \n",
    "    TRAINED_MODEL_PATH = os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_v2_best.pt')\n",
    "    \n",
    "    model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "    # Prediction\n",
    "    file_num_lst = []\n",
    "    pred_lst = []\n",
    "    prob_lst = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (img, file_num) in enumerate(test_dataloader):\n",
    "            img = img.to(device)\n",
    "            pred = F.softmax(model(img))\n",
    "            file_num_lst.extend(list(file_num))\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "    df = pd.DataFrame({'file_name':list(map(int,file_num_lst)), 'answer':pred_lst, 'prob':prob_lst})\n",
    "    df.sort_values(by=['file_name'], inplace=True)\n",
    "    df.to_csv(os.path.join(RESULT_DIR, f'mask_pred_with_{BACKBONE}_SAM_AdamP_v2.csv'), index=False)\n",
    "\n",
    "    trainer.validate_epoch(check_dataloader, 0)\n",
    "    \n",
    "    # save data\n",
    "    \n",
    "    TIME = datetime.now().strftime('%m%d%H%M%S')\n",
    "\n",
    "    save_dict = {'backbone':BACKBONE,\n",
    "                 'time':TIME,\n",
    "                 'best_val_loss':best_val_loss,\n",
    "                 'best_val_acc':best_val_acc,\n",
    "                 'test_acc':None,\n",
    "                 'epochs':EPOCHS,\n",
    "                 'input_shape':INPUT_SHAPE,\n",
    "                 'augmentation':None,\n",
    "                 'optimizer':'SAM, AdamP',\n",
    "                 'scheduler':'CosineAnnealingWarmRestarts',\n",
    "                 'attention_module':None,\n",
    "                 'learning_rate':LEARNING_RATE,\n",
    "                 'batch_size':BATCH_SIZE,\n",
    "                 'loss':'CrossEntropyLoss',\n",
    "                 'freeze':None,\n",
    "                 'others':None,\n",
    "                 'randomeseed':seed,\n",
    "                 'train_loss':train_loss_history,\n",
    "                 'train_acc':train_acc_history,\n",
    "                 'valid_loss':valid_loss_history,\n",
    "                 'valid_acc':valid_acc_history}\n",
    "\n",
    "\n",
    "    with open(f\"{os.path.join(NUMPY_DIR, BACKBONE)}_SAM_AdamP_v2_{TIME}.json\", 'w') as f:\n",
    "        json.dump(save_dict, f)\n",
    "    \n",
    "    wait_and_flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4d333-8cfa-4039-a9ef-865f07ec6956",
   "metadata": {},
   "source": [
    "# Debugging Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5354a12-0eec-4f97-84bc-fc3b0479aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEBUG_train_val_infer(BACKBONE=\"debug\"):\n",
    "    model = DebuggerNet().to(device)\n",
    "\n",
    "    # For Multi-GPU\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1'\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "    # Set optimizer, scheduler, loss function, metric function\n",
    "    base_optimizer = AdamP\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler =  optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=6, T_mult=1, eta_min=1e-6)\n",
    "    # criterion = TaylorCrossEntropyLoss(n=2, smoothing=0.1)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric_fn = get_metric_fn\n",
    "\n",
    "    # Set system logger\n",
    "    system_logger = get_logger(name='train',file_path='train_log.log')\n",
    "\n",
    "    # Set trainer\n",
    "    scales = [\n",
    "        [140, 70], [180, 90], [220, 110], \n",
    "        [260, 130], [300, 150], [340, 170],\n",
    "        [380, 190], [420, 210], [460, 230]\n",
    "             ]\n",
    "\n",
    "    trainer = SAM_Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)\n",
    "    # trainer = MultiScale_Trainer(criterion, model, device, metric_fn, scales, optimizer, scheduler, logger=system_logger)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    valid_loss_history = []\n",
    "    valid_acc_history = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_acc = None\n",
    "\n",
    "    \n",
    "    for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "        trainer.train_epoch(train_dataloader, epoch_index)\n",
    "        trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "        train_loss_history.append(trainer.train_mean_loss)\n",
    "        train_acc_history.append(trainer.train_score)\n",
    "\n",
    "        valid_loss_history.append(trainer.val_mean_loss)\n",
    "        valid_acc_history.append(trainer.validation_score)\n",
    "\n",
    "        # don't use early stopper\n",
    "\n",
    "        if best_val_loss == None or trainer.val_mean_loss < best_val_loss:\n",
    "            best_val_loss = trainer.val_mean_loss\n",
    "            criterion = trainer.val_mean_loss\n",
    "            check_point = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(check_point, os.path.join(WEIGHT_DIR, f'{BACKBONE}_best.pt'))\n",
    "\n",
    "    best_val_acc = max(valid_acc_history)\n",
    "\n",
    "    plot_loss(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    plot_acc(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    \n",
    "    TRAINED_MODEL_PATH = os.path.join(WEIGHT_DIR, f'{BACKBONE}_best.pt')\n",
    "    \n",
    "    model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "    # Prediction\n",
    "    file_num_lst = []\n",
    "    pred_lst = []\n",
    "    prob_lst = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (img, file_num) in enumerate(test_dataloader):\n",
    "            img = img.to(device)\n",
    "            pred = F.softmax(model(img))\n",
    "            file_num_lst.extend(list(file_num))\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "    df = pd.DataFrame({'file_name':list(map(int,file_num_lst)), 'answer':pred_lst, 'prob':prob_lst})\n",
    "    df.sort_values(by=['file_name'], inplace=True)\n",
    "    df.to_csv(os.path.join(RESULT_DIR, f'mask_pred_with_{BACKBONE}.csv'), index=False)\n",
    "\n",
    "    trainer.validate_epoch(check_dataloader, 0)\n",
    "    \n",
    "    # save data\n",
    "    \n",
    "    TIME = datetime.now().strftime('%m%d%H%M%S')\n",
    "\n",
    "    save_dict = {'backbone':BACKBONE,\n",
    "                 'time':TIME,\n",
    "                 'best_val_loss':best_val_loss,\n",
    "                 'best_val_acc':best_val_acc,\n",
    "                 'test_acc':None,\n",
    "                 'epochs':EPOCHS,\n",
    "                 'input_shape':INPUT_SHAPE,\n",
    "                 'augmentation':None,\n",
    "                 'optimizer':'Adam',\n",
    "                 'scheduler':'CosineAnnealingWarmRestarts',\n",
    "                 'attention_module':None,\n",
    "                 'learning_rate':LEARNING_RATE,\n",
    "                 'batch_size':BATCH_SIZE,\n",
    "                 'loss':'CrossEntropyLoss',\n",
    "                 'freeze':None,\n",
    "                 'others':None,\n",
    "                 'randomeseed':seed,\n",
    "                 'train_loss':train_loss_history,\n",
    "                 'train_acc':train_acc_history,\n",
    "                 'valid_loss':valid_loss_history,\n",
    "                 'valid_acc':valid_acc_history}\n",
    "\n",
    "\n",
    "    with open(f\"{os.path.join(NUMPY_DIR, BACKBONE)}_{TIME}.json\", 'w') as f:\n",
    "        json.dump(save_dict, f)\n",
    "    \n",
    "    wait_and_flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7beec4f4-4449-4ecd-a066-53468a356ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG_train_val_infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2efef2-1f51-46f9-8998-244846297a52",
   "metadata": {},
   "source": [
    "# 기차 = Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "493e3052-9938-4e15-a885-3e2f91109ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13710a-5b6a-4b68-825d-1fc07e768490",
   "metadata": {},
   "source": [
    "# Efficient Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07ccf8c6-28e9-4e41-ae01-24e14330eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['tf_efficientnetv2_m_in21k']\n"
     ]
    }
   ],
   "source": [
    "print(len(EFF_BACKBONES))\n",
    "print(EFF_BACKBONES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f6126c3-1c68-4080-8a28-202b53667da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6070207d-1cd1-4234-9686-2fb7562717c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 0.2436670553100508, Acc: 0.8975315729047072, ROC: 0.9631752524945195\n",
      "Epoch 0, Val loss: 0.13288515201113985, Acc: 0.9546125461254612, ROC: 0.9916797118159796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [11:13<3:33:13, 673.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.11776280706611479, Acc: 0.9598163030998852, ROC: 0.9927898262011161\n",
      "Epoch 1, Val loss: 0.10793667254671896, Acc: 0.9630996309963099, ROC: 0.994872963744609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [22:22<3:21:20, 671.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.07932802548221364, Acc: 0.9754387403641135, ROC: 0.9966026366287575\n",
      "Epoch 2, Val loss: 0.07795022250864016, Acc: 0.9719557195571956, ROC: 0.9963352836504944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [33:31<3:09:48, 669.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.05859174867502225, Acc: 0.9823273741184189, ROC: 0.9981854912897443\n",
      "Epoch 3, Val loss: 0.06743492249030202, Acc: 0.9767527675276753, ROC: 0.99718570292503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [44:41<2:58:38, 669.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train loss: 0.04396741025403736, Acc: 0.9867147777595539, ROC: 0.999024793423458\n",
      "Epoch 4, Val loss: 0.05652729156251429, Acc: 0.981549815498155, ROC: 0.997749676913123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [55:44<2:46:50, 667.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train loss: 0.026709181223669044, Acc: 0.9924143021157946, ROC: 0.99968167177767\n",
      "Epoch 5, Val loss: 0.04951755103781132, Acc: 0.9856088560885609, ROC: 0.9980578563749959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [1:06:52<2:35:46, 667.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train loss: 0.020527772967402824, Acc: 0.9949155322289651, ROC: 0.9998248351910931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [1:18:03<2:24:52, 668.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Val loss: 0.057224076693139135, Acc: 0.9782287822878228, ROC: 0.9979112900124777\n",
      "Epoch 7, Train loss: 0.02707467176868205, Acc: 0.9926193209775299, ROC: 0.99966560571479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [1:29:05<2:13:20, 666.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Val loss: 0.049667353239137756, Acc: 0.9841328413284133, ROC: 0.9977931452639839\n",
      "Epoch 8, Train loss: 0.018989653086282966, Acc: 0.9948745284566181, ROC: 0.9998513321538985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [1:40:14<2:02:21, 667.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Val loss: 0.050311955750916265, Acc: 0.985239852398524, ROC: 0.9979837372639125\n",
      "Epoch 9, Train loss: 0.014387931840729092, Acc: 0.9960226340823356, ROC: 0.9999124382371691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [1:51:14<1:50:50, 665.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Val loss: 0.05232564682938633, Acc: 0.9874538745387453, ROC: 0.9980528407960504\n",
      "Epoch 10, Train loss: 0.008863862829804232, Acc: 0.9974167623421355, ROC: 0.9999688793137448\n",
      "Epoch 10, Val loss: 0.0403517014163611, Acc: 0.9881918819188192, ROC: 0.9987277148074937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [2:02:22<1:39:54, 666.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Train loss: 0.0077954953317113515, Acc: 0.997744792520912, ROC: 0.9999694297570126\n",
      "Epoch 11, Val loss: 0.03947664864883401, Acc: 0.988929889298893, ROC: 0.998890999766497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [2:13:28<1:28:47, 665.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Train loss: 0.005617341112593723, Acc: 0.9983188453337707, ROC: 0.9999883581248853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [2:24:53<1:18:23, 671.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Val loss: 0.04432358988951173, Acc: 0.9900369003690037, ROC: 0.9983203383398099\n",
      "Epoch 13, Train loss: 0.007494343243708471, Acc: 0.9982778415614236, ROC: 0.9999676683385554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [2:36:03<1:07:08, 671.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Val loss: 0.0530105717038997, Acc: 0.9856088560885609, ROC: 0.9975730170769318\n",
      "Epoch 14, Train loss: 0.010868797857894696, Acc: 0.9967607019845826, ROC: 0.9999499715874947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [2:47:08<55:47, 669.41s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Val loss: 0.05428198134211807, Acc: 0.9833948339483395, ROC: 0.9978516603516813\n",
      "Epoch 15, Train loss: 0.00944351384587303, Acc: 0.9976217812038708, ROC: 0.9999482720939052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [2:58:23<44:43, 670.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Val loss: 0.05354445253510541, Acc: 0.9830258302583026, ROC: 0.997993211135254\n",
      "Epoch 16, Train loss: 0.006660891706229125, Acc: 0.9984828604231589, ROC: 0.9999797024044985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [3:09:30<33:29, 669.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Val loss: 0.05378042080902835, Acc: 0.9856088560885609, ROC: 0.9977585934979148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [3:16:51<34:44, 694.80s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-479f1843907f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEFF_train_val_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEFF_BACKBONES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mINDEX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-198b995232fc>\u001b[0m in \u001b[0;36mEFF_train_val_infer\u001b[0;34m(BACKBONE)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9e79e96cf8de>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, dataloader, epoch_index)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4b82510f155f>\u001b[0m in \u001b[0;36msecond_step\u001b[0;34m(self, zero_grad)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"e_w\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get back to \"w\" from \"w + e(w)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# do the actual \"sharpness-aware\" update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wd_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# Weight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36m_projection\u001b[0;34m(self, p, grad, perturb, delta, wd_ratio, eps)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mview_func\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_channel_view\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layer_view\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36m_cosine_similarity\u001b[0;34m(self, x, y, eps, view_func)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EFF_train_val_infer(EFF_BACKBONES[INDEX])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c01e1-55bf-49ff-939d-a842343dac4b",
   "metadata": {},
   "source": [
    "### NFNet Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3e93011b-9d80-422a-9028-df54f4968af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['eca_nfnet_l1', 'dm_nfnet_f2']\n"
     ]
    }
   ],
   "source": [
    "print(len(NFN_BACKBONES))\n",
    "print(NFN_BACKBONES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3bbae3b6-55dd-4046-b7f6-d087a760ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e632a700-f9ba-4566-83f9-3d8b86af1fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 0.16869012906223418, Acc: 0.9346399868787928, ROC: 0.9842188603165715\n",
      "Epoch 0, Val loss: 0.13439640543869966, Acc: 0.9564575645756458, ROC: 0.9942092354641555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [11:09<3:32:04, 669.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.074237793974051, Acc: 0.9760537969493194, ROC: 0.9968409441610259\n",
      "Epoch 1, Val loss: 0.07295935647562146, Acc: 0.9771217712177122, ROC: 0.9976549381997081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [22:23<3:21:41, 672.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.04481957442209056, Acc: 0.986837789076595, ROC: 0.9987949764785959\n",
      "Epoch 2, Val loss: 0.05055647489295159, Acc: 0.9841328413284133, ROC: 0.9984223217783682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [33:36<3:10:31, 672.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train loss: 0.027794524261382673, Acc: 0.9927833360669182, ROC: 0.999577328375711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [44:47<2:59:11, 671.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Val loss: 0.055892063756321816, Acc: 0.9800738007380074, ROC: 0.998334827790097\n",
      "Epoch 4, Train loss: 0.01948139081563988, Acc: 0.9948745284566181, ROC: 0.999801310621934\n",
      "Epoch 4, Val loss: 0.04407707345981934, Acc: 0.9845018450184502, ROC: 0.9988960153454424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [56:01<2:48:12, 672.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train loss: 0.008075106898084132, Acc: 0.9977037887485649, ROC: 0.9999685834504882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [1:07:16<2:37:07, 673.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Val loss: 0.04507311729420395, Acc: 0.9870848708487084, ROC: 0.9985075866204417\n",
      "Epoch 6, Train loss: 0.005067277289345406, Acc: 0.9987288830572413, ROC: 0.999986245798845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [1:18:30<2:25:58, 673.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Val loss: 0.04606750252613241, Acc: 0.9845018450184502, ROC: 0.9984624464099322\n",
      "Epoch 7, Train loss: 0.010362524748205372, Acc: 0.9972937510250943, ROC: 0.9999436483704554\n",
      "Epoch 7, Val loss: 0.041416383189604354, Acc: 0.9870848708487084, ROC: 0.9987009650531178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [1:29:43<2:14:42, 673.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train loss: 0.006434077599771873, Acc: 0.9983188453337707, ROC: 0.9999825027846236\n",
      "Epoch 8, Val loss: 0.037646852912290966, Acc: 0.9881918819188192, ROC: 0.9991284038365834\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [1:40:57<2:03:27, 673.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train loss: 0.0039547271145436765, Acc: 0.9990159094636707, ROC: 0.9999886333465191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [1:52:06<1:52:01, 672.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Val loss: 0.05345895223191297, Acc: 0.9848708487084871, ROC: 0.9987160117899543\n",
      "Epoch 10, Train loss: 0.0018803119405430872, Acc: 0.9993849434147941, ROC: 0.999998596369667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [2:03:18<1:40:49, 672.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Val loss: 0.050880615436992266, Acc: 0.9874538745387453, ROC: 0.9986920484683257\n",
      "Epoch 11, Train loss: 0.0010983598651091073, Acc: 0.9997949811382647, ROC: 0.9999996972562027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [2:14:30<1:29:36, 672.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Val loss: 0.03979054898745771, Acc: 0.9918819188191882, ROC: 0.9991562681640586\n",
      "Epoch 12, Train loss: 0.0007778355511761505, Acc: 0.9998359849106118, ROC: 0.999999869269724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [2:25:40<1:18:20, 671.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Val loss: 0.05325504035741397, Acc: 0.9896678966789668, ROC: 0.9990381234155647\n",
      "Epoch 13, Train loss: 0.002187554191893426, Acc: 0.9995489585041823, ROC: 0.9999983830729008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [2:36:51<1:07:07, 671.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Val loss: 0.05799002640131318, Acc: 0.9863468634686346, ROC: 0.9983075207491715\n",
      "Epoch 14, Train loss: 0.003666908452706312, Acc: 0.9989339019189766, ROC: 0.999996607893362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [2:48:07<1:12:03, 720.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Val loss: 0.04399886849761685, Acc: 0.9885608856088561, ROC: 0.9990955239301631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Val loss: 0.4593419064913178, Acc: 0.8966861598440545, ROC: 0.949732428115016\n",
      "waiting...\n",
      "flushing...\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "NFN_train_val_infer(NFN_BACKBONES[INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e0e7608b-e65c-46a3-8680-79e133bb70d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 0.4968300081809049, Acc: 0.7620141052976874, ROC: 0.8434700041308014\n",
      "Epoch 0, Val loss: 0.3602464266077301, Acc: 0.8494464944649447, ROC: 0.9313746197215909\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [43:24<13:44:50, 2604.74s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-675108b0a68e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mINDEX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mNFN_train_val_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNFN_BACKBONES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mINDEX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-7a98b0836af2>\u001b[0m in \u001b[0;36mNFN_train_val_infer\u001b[0;34m(BACKBONE)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9e79e96cf8de>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, dataloader, epoch_index)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfirst_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msecond_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-4b82510f155f>\u001b[0m in \u001b[0;36msecond_step\u001b[0;34m(self, zero_grad)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"e_w\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get back to \"w\" from \"w + e(w)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# do the actual \"sharpness-aware\" update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mzero_grad\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                     \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd_ratio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'delta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'wd_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# Weight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/adamp/adamp.py\u001b[0m in \u001b[0;36m_projection\u001b[0;34m(self, p, grad, perturb, delta, wd_ratio, eps)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mcosine_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mcosine_sim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mp_n\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mperturb\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mp_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mview_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_n\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mperturb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INDEX = 1\n",
    "NFN_train_val_infer(NFN_BACKBONES[INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1accfb11-7da8-4bb9-917c-ba78eb732ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "01_mask_code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "023f93d14c934299be94113aa089ff2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c01333330ae4863af15c0057fc3ae56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71dbfe7632524d69b167b4d56d44ed41",
      "placeholder": "​",
      "style": "IPY_MODEL_ef0ccb555eb34b45a4c5af40649f68ce",
      "value": "100%"
     }
    },
    "2e1f1639316a49e4acccba120fb254d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c01333330ae4863af15c0057fc3ae56",
       "IPY_MODEL_48d64e6e8d294ebd974fd9e610e935d0",
       "IPY_MODEL_bb54549aef364adca5e067690cf7258c"
      ],
      "layout": "IPY_MODEL_c972d82225b84aa3aa6cb513952162b9"
     }
    },
    "48d64e6e8d294ebd974fd9e610e935d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce8f3ea6a7de46b08961e82f991b1a0e",
      "max": 21388428,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_023f93d14c934299be94113aa089ff2f",
      "value": 21388428
     }
    },
    "71dbfe7632524d69b167b4d56d44ed41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb54549aef364adca5e067690cf7258c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd2482a049e843f188adae6ee381101f",
      "placeholder": "​",
      "style": "IPY_MODEL_db323916e13f4d7da295d5ad05cf5c84",
      "value": " 20.4M/20.4M [00:00&lt;00:00, 71.6MB/s]"
     }
    },
    "bd2482a049e843f188adae6ee381101f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c972d82225b84aa3aa6cb513952162b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce8f3ea6a7de46b08961e82f991b1a0e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db323916e13f4d7da295d5ad05cf5c84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef0ccb555eb34b45a4c5af40649f68ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
