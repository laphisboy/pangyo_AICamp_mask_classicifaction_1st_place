{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c50f01a-72fd-4804-9f54-d77b52d3991f",
   "metadata": {
    "id": "6c50f01a-72fd-4804-9f54-d77b52d3991f"
   },
   "source": [
    "if os.path.isfile(os.path.join(TEST_ANNOT_DIR, 'test.pkl')):\n",
    "    test_db = pd.read_pickle(os.path.join(TEST_ANNOT_DIR, 'test.pkl'))\n",
    "else:\n",
    "    test_db = test_loader()\n",
    "    test_db.to_pickle(os.path.join(TEST_ANNOT_DIR, 'test.pkl'))# Pangyo AI Challenge 2021 - Mask Classification Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cc3947-2739-40e8-9f71-feedc73a020c",
   "metadata": {},
   "source": [
    "# Change Epoch Back to 20!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b",
   "metadata": {
    "id": "f51ba7c1-0393-47ec-89d9-f6f97072773b"
   },
   "source": [
    "## 라이브러리 호출 및 I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a",
   "metadata": {
    "executionInfo": {
     "elapsed": 5126,
     "status": "ok",
     "timestamp": 1630047280444,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "98a45c7e-10ca-4fd1-9fd4-6326313a631a"
   },
   "outputs": [],
   "source": [
    "import os, torch, copy, cv2, sys, random, logging\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from adamp import AdamP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe733f8-2e09-4197-8fa8-3d166c7b5101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1353084"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getpid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcd8a1e9-a121-4fd3-9af4-23f1bbc2d6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f9c4250-2257-404f-941d-58eff1e9eb38",
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1630047303454,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "8f9c4250-2257-404f-941d-58eff1e9eb38"
   },
   "outputs": [],
   "source": [
    "# # 시드(seed) 설정\n",
    "\n",
    "# RANDOM_SEED = 2021\n",
    "# torch.manual_seed(RANDOM_SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.backends.cudnn.benchmark = False\n",
    "# np.random.seed(RANDOM_SEED)\n",
    "# random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e573764-7fdc-4e57-9a0a-39833df1d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    \n",
    "seed = 100\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6AYoEZyP1V_V",
   "metadata": {
    "executionInfo": {
     "elapsed": 216,
     "status": "ok",
     "timestamp": 1630047300749,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "6AYoEZyP1V_V"
   },
   "outputs": [],
   "source": [
    "def get_logger(name: str, file_path: str, stream=False) -> logging.RootLogger:\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.INFO)\n",
    "\n",
    "    formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s')\n",
    "    stream_handler = logging.StreamHandler()\n",
    "    file_handler = logging.FileHandler(file_path)\n",
    "\n",
    "    stream_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    if stream:\n",
    "        logger.addHandler(stream_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954",
   "metadata": {
    "id": "3a6c255b-b30d-4ffd-a663-bc01a2c37954"
   },
   "source": [
    "## Argument Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9",
   "metadata": {
    "executionInfo": {
     "elapsed": 219,
     "status": "ok",
     "timestamp": 1630047307151,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "9d69a8bc-2e64-4de6-928f-4e16957f6af9"
   },
   "outputs": [],
   "source": [
    "# working directory 지정\n",
    "ROOT_PATH = './'\n",
    "TRAIN_DIR = os.path.join(ROOT_PATH, 'train')\n",
    "RESULT_DIR = os.path.join(ROOT_PATH, 'results')\n",
    "WEIGHT_DIR = os.path.join(ROOT_PATH, 'weights')\n",
    "NUMPY_DIR = os.path.join(ROOT_PATH, 'numpy')\n",
    "CSV_DIR = os.path.join(ROOT_PATH, 'csv')\n",
    "TEST_DIR = os.path.join(ROOT_PATH, 'test')\n",
    "TEST_ANNOT_DIR = os.path.join(ROOT_PATH, 'test_annot')\n",
    "\n",
    "if not os.path.isdir(RESULT_DIR):\n",
    "    os.makedirs(RESULT_DIR)\n",
    "\n",
    "# hyper-parameters\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "WEIGHT_DECAY = 1e-8\n",
    "EARLY_STOPPING_PATIENCE = 5\n",
    "INPUT_SHAPE = (480, 240)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44807b0-7788-49ec-aff2-c756e4513c5e",
   "metadata": {
    "id": "d44807b0-7788-49ec-aff2-c756e4513c5e"
   },
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05",
   "metadata": {
    "id": "a5b81fa5-3756-46aa-b3cb-6f19879aba05"
   },
   "source": [
    "#### Train & Validation Set loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04642777-c2e0-439b-9692-f6c571a86521",
   "metadata": {
    "executionInfo": {
     "elapsed": 479,
     "status": "ok",
     "timestamp": 1630047309743,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "04642777-c2e0-439b-9692-f6c571a86521"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, db, mode):\n",
    "\n",
    "        self.db = db\n",
    "        self.mode = mode\n",
    "        self.transform = transforms.Compose([\n",
    "                                transforms.Resize(INPUT_SHAPE),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5)])\n",
    "        \n",
    "        self.autoaugment = transforms.Compose([\n",
    "            transforms.RandomApply(nn.ModuleList([\n",
    "                transforms.AutoAugment()]), p=0.5),\n",
    "            transforms.RandomApply(nn.ModuleList([\n",
    "                transforms.AutoAugment()]), p=0.5),\n",
    "            transforms.ToTensor(), \n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "        trans_image = TF.crop(trans_image, 0,0,INPUT_SHAPE[1], INPUT_SHAPE[1])\n",
    "        trans_image = self.autoaugment(trans_image)\n",
    "        \n",
    "        return trans_image, data['label']\n",
    "\n",
    "    \n",
    "def data_loader(data_dir=TRAIN_DIR):\n",
    "    print('Loading ' + ' dataset..')\n",
    "    if not os.path.isdir(data_dir):\n",
    "        print(f'!!! Cannot find {data_dir}... !!!')\n",
    "        sys.exit()\n",
    "        \n",
    "    mask_image_list = os.listdir(os.path.join(data_dir, 'Mask'))\n",
    "    nomask_image_list = os.listdir(os.path.join(data_dir, 'NoMask'))\n",
    "    mask_image_list = [item for item in mask_image_list if item[-4:] == '.png']\n",
    "    nomask_image_list = [item for item in nomask_image_list  if item[-4:] == '.png']\n",
    "    mask_image_path = list(map(lambda x : os.path.join(data_dir, 'Mask', x), mask_image_list))\n",
    "    nomask_image_path = list(map(lambda x : os.path.join(data_dir, 'NoMask', x), nomask_image_list))\n",
    "\n",
    "    # encoding label (Mask : 1, No Mask : 0)\n",
    "    mask_df = pd.DataFrame({'img_path':mask_image_path, 'label':np.ones(len(mask_image_list))})\n",
    "    nomask_df = pd.DataFrame({'img_path':nomask_image_path, 'label':np.zeros(len(nomask_image_list))})\n",
    "    db = mask_df.append(nomask_df, ignore_index=True)\n",
    "    return db\n",
    "\n",
    "\n",
    "if os.path.isfile(os.path.join(TRAIN_DIR, 'total.pkl')):\n",
    "    db = pd.read_pickle(os.path.join(TRAIN_DIR, 'total.pkl'))\n",
    "else:\n",
    "    db = data_loader()\n",
    "    db.to_pickle(os.path.join(TRAIN_DIR, 'total.pkl'))\n",
    "    \n",
    "    \n",
    "# Do stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_idx, valid_idx= train_test_split(\n",
    "    np.arange(len(db)),\n",
    "    test_size=0.1,\n",
    "    shuffle=True,\n",
    "    stratify=db.label.values,\n",
    "    random_state=seed)\n",
    "\n",
    "train_db = db.iloc[train_idx]\n",
    "valid_db = db.iloc[valid_idx]\n",
    "\n",
    "train_db = train_db.reset_index()\n",
    "valid_db = valid_db.reset_index()\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(INPUT_SHAPE),\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07040416-e5dc-41f6-a567-7a779dd4ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set samples: 24388 Val set samples: 2710\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CustomDataset(db=train_db, mode='train')\n",
    "validation_dataset = CustomDataset(db=valid_db, mode='val')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print('Train set samples:',len(train_dataset),  'Val set samples:', len(validation_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b27520-c82c-4ec8-ae0b-119a79167f09",
   "metadata": {
    "id": "61b27520-c82c-4ec8-ae0b-119a79167f09"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d34769e-afab-46ef-9168-94189235e48d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5400,
     "status": "ok",
     "timestamp": 1630047317233,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "2d34769e-afab-46ef-9168-94189235e48d",
    "outputId": "c6b7da67-22cf-4835-b4ba-0dce62980eca"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/rwightman/pytorch-image-models.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6c37334-3edd-4687-a88e-f5948bc15ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7f96e9d-dfed-412d-9dd8-02b99970b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# timm.list_models(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a722d66-0cc7-4e49-9cdc-b90628407b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "EFF_BACKBONES = [\n",
    "    'tf_efficientnetv2_m_in21k'\n",
    "]\n",
    "NFN_BACKBONES = [\n",
    "    'eca_nfnet_l1',\n",
    "    'dm_nfnet_f1'    \n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84ff9566-36e8-4a7d-8e23-af605a8ec3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebuggerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DebuggerNet, self).__init__()\n",
    "        self.conv = nn.Conv2d(3, 3, 3, 3)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(2)\n",
    "        self.linear = nn.Linear(12, 2)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        x = self.avgpool(input_img)\n",
    "\n",
    "        x = x.view(-1, 12)\n",
    "        x = self.linear(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bef3b15-f18b-4aa9-b6a0-65822041b4e9",
   "metadata": {
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1630047325374,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "3bef3b15-f18b-4aa9-b6a0-65822041b4e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "# for efficientnet\n",
    "class EFFMaskClassifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(EFFMaskClassifier, self).__init__()\n",
    "        self.model = timm.create_model(backbone, pretrained=True)\n",
    "        n_features = self.model.classifier.in_features\n",
    "        self.model.classifier = nn.Linear(n_features, 2)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        x = self.model(input_img)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee11cfc8-9b26-4250-950a-3e02053009f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for nfnet\n",
    "class NFNMaskClassifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(NFNMaskClassifier, self).__init__()\n",
    "        self.model = timm.create_model(backbone, pretrained=True)\n",
    "        n_features = self.model.head.fc.in_features\n",
    "        self.model.head.fc = nn.Linear(n_features, 2)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        x = self.model(input_img)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f9fb549-609a-43e0-be73-95d9c9729713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for deit\n",
    "class DEITMaskClassifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(DEITMaskClassifier, self).__init__()\n",
    "        self.model = timm.create_model(backbone, pretrained=True)\n",
    "        in_features_head = self.model.head.in_features\n",
    "        in_features_head_dist = self.model.head_dist.in_features\n",
    "        \n",
    "        self.model.head = nn.Linear(in_features_head, 2)\n",
    "        self.model.head_dist = nn.Linear(in_features_head_dist, 2)\n",
    "\n",
    "    def forward(self, input_img):\n",
    "        x = self.model(input_img)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba438c6-44d1-4960-be0c-570d97bbb7e3",
   "metadata": {},
   "source": [
    "## Just Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3345b82-115e-4130-b53b-706d30c20dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion, model, device, metric_fn, optimizer=None, scheduler=None, logger=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            loss = self.criterion(pred, label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            pred = F.softmax(pred)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            train_total_loss += loss.item()\n",
    "            \n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "        \n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            pred = F.softmax(pred)\n",
    "            ## coordinate loss\n",
    "            loss = self.criterion(pred, label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, ROC: {auroc}'\n",
    "        print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591a5722-df20-4d90-9be8-f1c98fa31009",
   "metadata": {},
   "source": [
    "## SAM Optimizer w. AdamP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5fbbd9cf-af12-46ab-9365-39e775e22781",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM(torch.optim.Optimizer):\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, **kwargs):\n",
    "        assert rho >= 0.0, f\"Invalid rho, should be non-negative: {rho}\"\n",
    "\n",
    "        defaults = dict(rho=rho, **kwargs)\n",
    "        super(SAM, self).__init__(params, defaults)\n",
    "\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "        self.param_groups = self.base_optimizer.param_groups\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=False):\n",
    "        grad_norm = self._grad_norm()\n",
    "        for group in self.param_groups:\n",
    "            scale = group[\"rho\"] / (grad_norm + 1e-12)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                e_w = p.grad * scale.to(p)\n",
    "                p.add_(e_w)  # climb to the local maximum \"w + e(w)\"\n",
    "                self.state[p][\"e_w\"] = e_w\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=False):\n",
    "        for group in self.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p][\"e_w\"])  # get back to \"w\" from \"w + e(w)\"\n",
    "\n",
    "        self.base_optimizer.step()  # do the actual \"sharpness-aware\" update\n",
    "\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        assert closure is not None, \"Sharpness Aware Minimization requires closure, but it was not provided\"\n",
    "        closure = torch.enable_grad()(closure)  # the closure should do a full forward-backward pass\n",
    "\n",
    "        self.first_step(zero_grad=True)\n",
    "        closure()\n",
    "        self.second_step()\n",
    "\n",
    "    def _grad_norm(self):\n",
    "        shared_device = self.param_groups[0][\"params\"][0].device  # put everything on the same device, in case of model parallelism\n",
    "        norm = torch.norm(\n",
    "                    torch.stack([\n",
    "                        p.grad.norm(p=2).to(shared_device)\n",
    "                        for group in self.param_groups for p in group[\"params\"]\n",
    "                        if p.grad is not None\n",
    "                    ]),\n",
    "                    p=2\n",
    "               )\n",
    "        return norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aaffd8d-b025-42c1-8dd8-69529487389e",
   "metadata": {
    "id": "1aaffd8d-b025-42c1-8dd8-69529487389e"
   },
   "source": [
    "## MultiScale Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5faaac1b-64c3-4659-82de-d4309502f29a",
   "metadata": {
    "executionInfo": {
     "elapsed": 277,
     "status": "ok",
     "timestamp": 1630047330626,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "5faaac1b-64c3-4659-82de-d4309502f29a"
   },
   "outputs": [],
   "source": [
    "class MultiScale_Trainer():\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion, model, device, metric_fn, scales, optimizer=None, scheduler=None, logger=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.scales = scales\n",
    "        self.scales_max_index = len(self.scales) - 1\n",
    "        self.criterion = criterion\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            \n",
    "            img = TF.resize(img, self.scales[np.random.randint(0, self.scales_max_index)])\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            loss = self.criterion(pred, label)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            ## coordinate loss\n",
    "            loss = self.criterion(pred, label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8397f3c-bb85-4bf5-928c-ed4ce82ae232",
   "metadata": {},
   "source": [
    "# SAM Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ebf28b9-1118-417e-8122-02f6bfd33624",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAM_Trainer():\n",
    "    \"\"\" Trainer\n",
    "        epoch에 대한 학습 및 검증 절차 정의\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion, model, device, metric_fn, optimizer=None, scheduler=None, logger=None):\n",
    "        \"\"\" 초기화\n",
    "        \"\"\"\n",
    "        \n",
    "        self.criterion = criterion\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.logger = logger\n",
    "        self.scheduler = scheduler\n",
    "        self.metric_fn = metric_fn\n",
    "\n",
    "    def train_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 학습 절차\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "        \n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            loss = self.criterion(pred, label)\n",
    "            loss.backward()\n",
    "            self.optimizer.first_step(zero_grad=True)\n",
    "            self.criterion(self.model(img), label).backward()\n",
    "            self.optimizer.second_step(zero_grad=True)\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            train_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.train_mean_loss = train_total_loss / batch_index\n",
    "        self.train_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Train loss: {self.train_mean_loss}, Acc: {self.train_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "\n",
    "    def validate_epoch(self, dataloader, epoch_index):\n",
    "        \"\"\" 한 epoch에서 수행되는 검증 절차\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        val_total_loss = 0\n",
    "        target_lst = []\n",
    "        pred_lst = []\n",
    "        prob_lst = []\n",
    "\n",
    "        for batch_index, (img, label) in enumerate(dataloader):\n",
    "            img = img.to(self.device)\n",
    "            label = label.to(self.device).long()\n",
    "            pred = self.model(img)\n",
    "            ## coordinate loss\n",
    "            loss = self.criterion(pred, label)\n",
    "            val_total_loss += loss.item()\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            target_lst.extend(label.cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "        self.val_mean_loss = val_total_loss / batch_index\n",
    "        self.validation_score, auroc = self.metric_fn(y_pred=pred_lst, y_answer=target_lst, y_prob=prob_lst)\n",
    "        msg = f'Epoch {epoch_index}, Val loss: {self.val_mean_loss}, Acc: {self.validation_score}, ROC: {auroc}'\n",
    "        print(msg)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aca506-d168-4c9f-8eca-5cdecb122961",
   "metadata": {
    "id": "e2aca506-d168-4c9f-8eca-5cdecb122961"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33678d90-a254-48d5-bf09-2a817eeafea3",
   "metadata": {
    "executionInfo": {
     "elapsed": 799,
     "status": "ok",
     "timestamp": 1630047333382,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "33678d90-a254-48d5-bf09-2a817eeafea3"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "def get_metric_fn(y_pred, y_answer, y_prob):\n",
    "    \"\"\" 성능을 반환하는 함수\n",
    "    \"\"\"\n",
    "    assert len(y_pred) == len(y_answer), 'The size of prediction and answer are not same.'\n",
    "    accuracy = accuracy_score(y_answer, y_pred)\n",
    "    auroc = roc_auc_score(y_answer, y_prob)\n",
    "    return accuracy, auroc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c20b881c-efdd-4512-bf48-bb36a6c34672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaylorSoftmax(nn.Module):\n",
    "\n",
    "    def __init__(self, dim=1, n=2):\n",
    "        super(TaylorSoftmax, self).__init__()\n",
    "        assert n % 2 == 0\n",
    "        self.dim = dim\n",
    "        self.n = n\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        fn = torch.ones_like(x)\n",
    "        denor = 1.\n",
    "        for i in range(1, self.n+1):\n",
    "            denor *= i\n",
    "            fn = fn + x.pow(i) / denor\n",
    "        out = fn / fn.sum(dim=self.dim, keepdims=True)\n",
    "        return out\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1): \n",
    "        super(LabelSmoothingLoss, self).__init__() \n",
    "        self.confidence = 1.0 - smoothing \n",
    "        self.smoothing = smoothing \n",
    "        self.cls = classes \n",
    "        self.dim = dim \n",
    "    def forward(self, pred, target): \n",
    "        \"\"\"Taylor Softmax and log are already applied on the logits\"\"\"\n",
    "        #pred = pred.log_softmax(dim=self.dim) \n",
    "        with torch.no_grad(): \n",
    "            true_dist = torch.zeros_like(pred) \n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1)) \n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence) \n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n",
    "    \n",
    "class TaylorCrossEntropyLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, n=2, ignore_index=-1, reduction='mean', smoothing=0.1):\n",
    "        super(TaylorCrossEntropyLoss, self).__init__()\n",
    "        assert n % 2 == 0\n",
    "        self.taylor_softmax = TaylorSoftmax(dim=1, n=n)\n",
    "        self.reduction = reduction\n",
    "        self.ignore_index = ignore_index\n",
    "        self.lab_smooth = LabelSmoothingLoss(2, smoothing=smoothing)\n",
    "\n",
    "    def forward(self, logits, labels):\n",
    "\n",
    "        log_probs = self.taylor_softmax(logits).log()\n",
    "        #loss = F.nll_loss(log_probs, labels, reduction=self.reduction,\n",
    "        #        ignore_index=self.ignore_index)\n",
    "        loss = self.lab_smooth(log_probs, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331",
   "metadata": {
    "id": "75bbba92-b53c-499f-b5f9-b6ac3edde331"
   },
   "source": [
    "#### Test set Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a60a4c6-306e-499d-b071-2ab39b795b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ANNOT_DIR = os.path.join(ROOT_PATH, 'test_annot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a52193e8-3033-4d4b-9d00-754be781170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckDataset(Dataset):\n",
    "    def __init__(self, db, mode, transform):\n",
    "\n",
    "        self.db = db\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "\n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff2e6cb0-159f-4a25-bb72-779a09b4ba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_loader(data_dir=TEST_ANNOT_DIR):\n",
    "    print('Loading ' + ' dataset..')\n",
    "    if not os.path.isdir(data_dir):\n",
    "        print(f'!!! Cannot find {data_dir}... !!!')\n",
    "        sys.exit()\n",
    "        \n",
    "    mask_image_list = os.listdir(os.path.join(data_dir, 'Mask'))\n",
    "    nomask_image_list = os.listdir(os.path.join(data_dir, 'NoMask'))\n",
    "    mask_image_list = [item for item in mask_image_list if item[-4:] == '.png']\n",
    "    nomask_image_list = [item for item in nomask_image_list  if item[-4:] == '.png']\n",
    "    mask_image_path = list(map(lambda x : os.path.join(data_dir, 'Mask', x), mask_image_list))\n",
    "    nomask_image_path = list(map(lambda x : os.path.join(data_dir, 'NoMask', x), nomask_image_list))\n",
    "\n",
    "    # encoding label (Mask : 1, No Mask : 0)\n",
    "    mask_df = pd.DataFrame({'img_path':mask_image_path, 'label':np.ones(len(mask_image_list))})\n",
    "    nomask_df = pd.DataFrame({'img_path':nomask_image_path, 'label':np.zeros(len(nomask_image_list))})\n",
    "    db = mask_df.append(nomask_df, ignore_index=True)\n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30a3f76d-abfe-4a26-a15a-8626de7221ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(os.path.join(TEST_ANNOT_DIR, 'test.pkl')):\n",
    "    test_db = pd.read_pickle(os.path.join(TEST_ANNOT_DIR, 'test.pkl'))\n",
    "else:\n",
    "    test_db = check_loader()\n",
    "    test_db.to_pickle(os.path.join(TEST_ANNOT_DIR, 'test.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79d51d04-ea62-466d-b988-4e782c9aa5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, data_dir, input_shape):\n",
    "        self.data_dir = data_dir\n",
    "        self.input_shape = input_shape\n",
    "        \n",
    "        # Loading dataset\n",
    "        self.db = self.data_loader()\n",
    "        \n",
    "        # Transform function\n",
    "        self.transform = transforms.Compose([transforms.Resize(self.input_shape), transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "    def data_loader(self):\n",
    "        print('Loading test dataset..')\n",
    "        if not os.path.isdir(self.data_dir):\n",
    "            print(f'!!! Cannot find {self.data_dir}... !!!')\n",
    "            sys.exit()\n",
    "        image_list = os.listdir(self.data_dir)\n",
    "        image_list = [item for item in image_list if item[-4:] == '.png']\n",
    "        image_path = list(map(lambda x : os.path.join(self.data_dir, x), image_list))\n",
    "        db = pd.DataFrame({'img_path':image_path, 'file_num':list(map(lambda x : x.split('.')[0], image_list))})\n",
    "        return db\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.db)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        data = copy.deepcopy(self.db.loc[index])\n",
    "        \n",
    "        # Loading image\n",
    "        cvimg = cv2.imread(data['img_path'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n",
    "        if not isinstance(cvimg, np.ndarray):\n",
    "            raise IOError(\"Fail to read %s\" % data['img_path'])\n",
    "\n",
    "        # Preprocessing images\n",
    "        trans_image = self.transform(Image.fromarray(cvimg))\n",
    "\n",
    "        return trans_image, data['file_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "078c24da-fa47-4b4f-a36c-db1c961143b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.Resize(INPUT_SHAPE),\n",
    "                                transforms.ToTensor(), \n",
    "                                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6127,
     "status": "ok",
     "timestamp": 1630047496452,
     "user": {
      "displayName": "박기돈",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhVEYQCp6v5ebpmCrZSsSB10iVBTuKIfM20WSM_=s64",
      "userId": "12822433786877269578"
     },
     "user_tz": -540
    },
    "id": "cdd31a3d-08cd-48fc-87b0-137976d4d4bb",
    "outputId": "191d15b8-07ab-4d56-a2af-071ec0ec4b8c"
   },
   "outputs": [],
   "source": [
    "check_dataset = CheckDataset(db=test_db, mode='check', transform=transform)\n",
    "check_dataloader = DataLoader(check_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ac79993-ec06-425d-8934-dad2f1d2ae80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test dataset..\n"
     ]
    }
   ],
   "source": [
    "test_dataset = TestDataset(data_dir=TEST_DIR, input_shape=INPUT_SHAPE)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d796ac-7b07-44ac-b453-40699fbc1336",
   "metadata": {},
   "source": [
    "# Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e60e2016-ef06-4ae9-8ac1-948347c4b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(train_loss_history, valid_loss_history, backbone):\n",
    "    plt.plot(train_loss_history, label=\"train loss\")\n",
    "    plt.plot(valid_loss_history, label=\"valid loss\")\n",
    "    plt.legend()\n",
    "    plt.title(backbone)\n",
    "    plt.show()\n",
    "\n",
    "def plot_acc(train_acc_history, valid_acc_history, backbone):\n",
    "    plt.plot(train_acc_history, label=\"train acc\")\n",
    "    plt.plot(valid_acc_history, label=\"valid acc\")\n",
    "    plt.legend()\n",
    "    plt.title(backbone)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c41f1889-f394-4226-b535-5c9e554db26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_and_flush():\n",
    "    \n",
    "    print(\"waiting...\")\n",
    "    \n",
    "    time.sleep(120)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"flushing...\")\n",
    "    time.sleep(60)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5fcfeb6-c198-4906-90c5-5254bbb36bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_size(size):\n",
    "\n",
    "    \"\"\"Pretty prints a torch.Size object\"\"\"\n",
    "    assert(isinstance(size, torch.Size))\n",
    "    return \" × \".join(map(str, size))\n",
    "\n",
    "def dump_tensors(gpu_only=True):\n",
    "    \"\"\"Prints a list of the Tensors being tracked by the garbage collector.\"\"\"\n",
    "    import gc\n",
    "    total_size = 0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s:%s%s %s\" % (type(obj).__name__, \n",
    "                                          \" GPU\" if obj.is_cuda else \"\",\n",
    "                                          \" pinned\" if obj.is_pinned else \"\",\n",
    "                                          pretty_size(obj.size())))\n",
    "                    total_size += obj.numel()\n",
    "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\"%s → %s:%s%s%s%s %s\" % (type(obj).__name__, \n",
    "                                                   type(obj.data).__name__, \n",
    "                                                   \" GPU\" if obj.is_cuda else \"\",\n",
    "                                                   \" pinned\" if obj.data.is_pinned else \"\",\n",
    "                                                   \" grad\" if obj.requires_grad else \"\", \n",
    "                                                   \" volatile\" if obj.volatile else \"\",\n",
    "                                                   pretty_size(obj.data.size())))\n",
    "                    total_size += obj.data.numel()\n",
    "        except Exception as e:\n",
    "            pass        \n",
    "    print(\"Total size:\", total_size)\n",
    "\n",
    "# dump_tensors()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00409945-7c70-4633-8923-1e48d17053de",
   "metadata": {},
   "source": [
    "# 학습 Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a9f17d1e-e9ee-4b9f-ab39-6d6b15073c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EFF_train_val_infer(BACKBONE):\n",
    "    model = EFFMaskClassifier(BACKBONE).to(device)\n",
    "\n",
    "    base_optimizer = AdamP\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler =  optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=5, T_mult=2, eta_min=1e-5, last_epoch=-1)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric_fn = get_metric_fn\n",
    "\n",
    "    # Set system logger\n",
    "    system_logger = get_logger(name='train',file_path='train_log.log')\n",
    "\n",
    "    trainer = SAM_Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)\n",
    "    # trainer = MultiScale_Trainer(criterion, model, device, metric_fn, scales, optimizer, scheduler, logger=system_logger)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    valid_loss_history = []\n",
    "    valid_acc_history = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_acc = None\n",
    "    \n",
    "    patience = 0\n",
    "    \n",
    "    for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "        trainer.train_epoch(train_dataloader, epoch_index)\n",
    "        trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "        train_loss_history.append(trainer.train_mean_loss)\n",
    "        train_acc_history.append(trainer.train_score)\n",
    "\n",
    "        valid_loss_history.append(trainer.val_mean_loss)\n",
    "        valid_acc_history.append(trainer.validation_score)\n",
    "\n",
    "        # don't use early stopper\n",
    "        patience += 1\n",
    "        \n",
    "        if best_val_loss == None or trainer.val_mean_loss < best_val_loss:\n",
    "            best_val_loss = trainer.val_mean_loss\n",
    "            patience = 0\n",
    "            check_point = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(check_point, os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_AA_best.pt'))\n",
    "        \n",
    "        if patience == EARLY_STOPPING_PATIENCE:\n",
    "            break\n",
    "            \n",
    "    best_val_acc = max(valid_acc_history)\n",
    "\n",
    "    plot_loss(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    plot_acc(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    \n",
    "    TRAINED_MODEL_PATH = os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_AA_best.pt')\n",
    "    \n",
    "    model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "    # Prediction\n",
    "    file_num_lst = []\n",
    "    pred_lst = []\n",
    "    prob_lst = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (img, file_num) in enumerate(test_dataloader):\n",
    "            img = img.to(device)\n",
    "            pred = model(img)\n",
    "            pred = F.softmax(pred)\n",
    "            file_num_lst.extend(list(file_num))\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'file_name':list(map(int,file_num_lst)), 'answer':pred_lst, 'prob':prob_lst})\n",
    "    df.sort_values(by=['file_name'], inplace=True)\n",
    "    df.to_csv(os.path.join(RESULT_DIR, f'mask_pred_with_{BACKBONE}_SAM_AdamP_AA.csv'), index=False)\n",
    "\n",
    "    trainer.validate_epoch(check_dataloader, 0)\n",
    "    \n",
    "    # save data\n",
    "    \n",
    "    TIME = datetime.now().strftime('%m%d%H%M%S')\n",
    "\n",
    "    save_dict = {'backbone':BACKBONE,\n",
    "                 'time':TIME,\n",
    "                 'best_val_loss':best_val_loss,\n",
    "                 'best_val_acc':best_val_acc,\n",
    "                 'test_acc':None,\n",
    "                 'epochs':EPOCHS,\n",
    "                 'input_shape':INPUT_SHAPE,\n",
    "                 'augmentation':None,\n",
    "                 'optimizer':'SAM, AdamP',\n",
    "                 'scheduler':'CosineAnnealingWarmRestarts',\n",
    "                 'attention_module':None,\n",
    "                 'learning_rate':LEARNING_RATE,\n",
    "                 'batch_size':BATCH_SIZE,\n",
    "                 'loss':'CrossEntropyLoss',\n",
    "                 'freeze':None,\n",
    "                 'others':None,\n",
    "                 'randomeseed':seed,\n",
    "                 'train_loss':train_loss_history,\n",
    "                 'train_acc':train_acc_history,\n",
    "                 'valid_loss':valid_loss_history,\n",
    "                 'valid_acc':valid_acc_history}\n",
    "\n",
    "\n",
    "    with open(f\"{os.path.join(NUMPY_DIR, BACKBONE)}_SAM_AdamP_AA_{TIME}.json\", 'w') as f:\n",
    "        json.dump(save_dict, f)\n",
    "\n",
    "    wait_and_flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf39f4e9-6b00-4bc6-ab84-85863b98ec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NFN_train_val_infer(BACKBONE):\n",
    "    model = NFNMaskClassifier(BACKBONE).to(device)\n",
    "\n",
    "    base_optimizer = AdamP\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler =  optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=5, T_mult=2, eta_min=1e-5, last_epoch=-1)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric_fn = get_metric_fn\n",
    "\n",
    "    # Set system logger\n",
    "    system_logger = get_logger(name='train',file_path='train_log.log')\n",
    "\n",
    "    trainer = SAM_Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)\n",
    "    # trainer = MultiScale_Trainer(criterion, model, device, metric_fn, scales, optimizer, scheduler, logger=system_logger)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    valid_loss_history = []\n",
    "    valid_acc_history = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_acc = None\n",
    "    \n",
    "    patience = 0\n",
    "    \n",
    "    for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "        trainer.train_epoch(train_dataloader, epoch_index)\n",
    "        trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "        train_loss_history.append(trainer.train_mean_loss)\n",
    "        train_acc_history.append(trainer.train_score)\n",
    "\n",
    "        valid_loss_history.append(trainer.val_mean_loss)\n",
    "        valid_acc_history.append(trainer.validation_score)\n",
    "\n",
    "        # don't use early stopper\n",
    "        patience += 1\n",
    "        \n",
    "        if best_val_loss == None or trainer.val_mean_loss < best_val_loss:\n",
    "            best_val_loss = trainer.val_mean_loss\n",
    "            patience = 0\n",
    "            check_point = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(check_point, os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_AA_best.pt'))\n",
    "        \n",
    "        if patience == EARLY_STOPPING_PATIENCE:\n",
    "            break\n",
    "            \n",
    "    best_val_acc = max(valid_acc_history)\n",
    "\n",
    "    plot_loss(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    plot_acc(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    \n",
    "    TRAINED_MODEL_PATH = os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_AA_best.pt')\n",
    "    \n",
    "    model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "    # Prediction\n",
    "    file_num_lst = []\n",
    "    pred_lst = []\n",
    "    prob_lst = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (img, file_num) in enumerate(test_dataloader):\n",
    "            img = img.to(device)\n",
    "            pred = model(img)\n",
    "            pred = F.softmax(pred)\n",
    "            file_num_lst.extend(list(file_num))\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "\n",
    "    df = pd.DataFrame({'file_name':list(map(int,file_num_lst)), 'answer':pred_lst, 'prob':prob_lst})\n",
    "    df.sort_values(by=['file_name'], inplace=True)\n",
    "    df.to_csv(os.path.join(RESULT_DIR, f'mask_pred_with_{BACKBONE}_SAM_AdamP_AA.csv'), index=False)\n",
    "\n",
    "    trainer.validate_epoch(check_dataloader, 0)\n",
    "    \n",
    "    # save data\n",
    "    \n",
    "    TIME = datetime.now().strftime('%m%d%H%M%S')\n",
    "\n",
    "    save_dict = {'backbone':BACKBONE,\n",
    "                 'time':TIME,\n",
    "                 'best_val_loss':best_val_loss,\n",
    "                 'best_val_acc':best_val_acc,\n",
    "                 'test_acc':None,\n",
    "                 'epochs':EPOCHS,\n",
    "                 'input_shape':INPUT_SHAPE,\n",
    "                 'augmentation':None,\n",
    "                 'optimizer':'SAM, AdamP',\n",
    "                 'scheduler':'CosineAnnealingWarmRestarts',\n",
    "                 'attention_module':None,\n",
    "                 'learning_rate':LEARNING_RATE,\n",
    "                 'batch_size':BATCH_SIZE,\n",
    "                 'loss':'CrossEntropyLoss',\n",
    "                 'freeze':None,\n",
    "                 'others':None,\n",
    "                 'randomeseed':seed,\n",
    "                 'train_loss':train_loss_history,\n",
    "                 'train_acc':train_acc_history,\n",
    "                 'valid_loss':valid_loss_history,\n",
    "                 'valid_acc':valid_acc_history}\n",
    "\n",
    "\n",
    "    with open(f\"{os.path.join(NUMPY_DIR, BACKBONE)}_SAM_AdamP_AA_{TIME}.json\", 'w') as f:\n",
    "        json.dump(save_dict, f)\n",
    "\n",
    "    wait_and_flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ad3aa24-d241-469d-b623-f79450ae0d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEIT_train_val_infer(BACKBONE):\n",
    "    model = DEITMaskClassifier(BACKBONE).to(device)\n",
    "\n",
    "    # For Multi-GPU\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1'\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "    # Set optimizer, scheduler, loss function, metric function\n",
    "    base_optimizer = AdamP\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler =  optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=6, T_mult=1, eta_min=1e-6)\n",
    "    # criterion = TaylorCrossEntropyLoss(n=2, smoothing=0.1)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric_fn = get_metric_fn\n",
    "\n",
    "    # Set system logger\n",
    "    system_logger = get_logger(name='train',file_path='train_log.log')\n",
    "\n",
    "    # Set trainer\n",
    "    scales = [\n",
    "        [140, 70], [180, 90], [220, 110], \n",
    "        [260, 130], [300, 150], [340, 170],\n",
    "        [380, 190], [420, 210], [460, 230]\n",
    "             ]\n",
    "\n",
    "    trainer = SAM_Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)\n",
    "    # trainer = MultiScale_Trainer(criterion, model, device, metric_fn, scales, optimizer, scheduler, logger=system_logger)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    valid_loss_history = []\n",
    "    valid_acc_history = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_acc = None\n",
    "\n",
    "    \n",
    "    for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "        trainer.train_epoch(train_dataloader, epoch_index)\n",
    "        trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "        train_loss_history.append(trainer.train_mean_loss)\n",
    "        train_acc_history.append(trainer.train_score)\n",
    "\n",
    "        valid_loss_history.append(trainer.val_mean_loss)\n",
    "        valid_acc_history.append(trainer.validation_score)\n",
    "\n",
    "        # don't use early stopper\n",
    "\n",
    "        if best_val_loss == None or trainer.val_mean_loss < best_val_loss:\n",
    "            best_val_loss = trainer.val_mean_loss\n",
    "            criterion = trainer.val_mean_loss\n",
    "            check_point = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(check_point, os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_v2_best.pt'))\n",
    "\n",
    "    best_val_acc = max(valid_acc_history)\n",
    "\n",
    "    plot_loss(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    plot_acc(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    \n",
    "    TRAINED_MODEL_PATH = os.path.join(WEIGHT_DIR, f'{BACKBONE}_SAM_AdamP_v2_best.pt')\n",
    "    \n",
    "    model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "    # Prediction\n",
    "    file_num_lst = []\n",
    "    pred_lst = []\n",
    "    prob_lst = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (img, file_num) in enumerate(test_dataloader):\n",
    "            img = img.to(device)\n",
    "            pred = F.softmax(model(img))\n",
    "            file_num_lst.extend(list(file_num))\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "    df = pd.DataFrame({'file_name':list(map(int,file_num_lst)), 'answer':pred_lst, 'prob':prob_lst})\n",
    "    df.sort_values(by=['file_name'], inplace=True)\n",
    "    df.to_csv(os.path.join(RESULT_DIR, f'mask_pred_with_{BACKBONE}_SAM_AdamP_v2.csv'), index=False)\n",
    "\n",
    "    trainer.validate_epoch(check_dataloader, 0)\n",
    "    \n",
    "    # save data\n",
    "    \n",
    "    TIME = datetime.now().strftime('%m%d%H%M%S')\n",
    "\n",
    "    save_dict = {'backbone':BACKBONE,\n",
    "                 'time':TIME,\n",
    "                 'best_val_loss':best_val_loss,\n",
    "                 'best_val_acc':best_val_acc,\n",
    "                 'test_acc':None,\n",
    "                 'epochs':EPOCHS,\n",
    "                 'input_shape':INPUT_SHAPE,\n",
    "                 'augmentation':None,\n",
    "                 'optimizer':'SAM, AdamP',\n",
    "                 'scheduler':'CosineAnnealingWarmRestarts',\n",
    "                 'attention_module':None,\n",
    "                 'learning_rate':LEARNING_RATE,\n",
    "                 'batch_size':BATCH_SIZE,\n",
    "                 'loss':'CrossEntropyLoss',\n",
    "                 'freeze':None,\n",
    "                 'others':None,\n",
    "                 'randomeseed':seed,\n",
    "                 'train_loss':train_loss_history,\n",
    "                 'train_acc':train_acc_history,\n",
    "                 'valid_loss':valid_loss_history,\n",
    "                 'valid_acc':valid_acc_history}\n",
    "\n",
    "\n",
    "    with open(f\"{os.path.join(NUMPY_DIR, BACKBONE)}_SAM_AdamP_v2_{TIME}.json\", 'w') as f:\n",
    "        json.dump(save_dict, f)\n",
    "    \n",
    "    wait_and_flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde4d333-8cfa-4039-a9ef-865f07ec6956",
   "metadata": {},
   "source": [
    "# Debugging Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b5354a12-0eec-4f97-84bc-fc3b0479aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DEBUG_train_val_infer(BACKBONE=\"debug\"):\n",
    "    model = DebuggerNet().to(device)\n",
    "\n",
    "    # For Multi-GPU\n",
    "#     os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0, 1'\n",
    "#     model = nn.DataParallel(model)\n",
    "\n",
    "    # Set optimizer, scheduler, loss function, metric function\n",
    "    base_optimizer = AdamP\n",
    "    optimizer = SAM(model.parameters(), base_optimizer, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler =  optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=6, T_mult=1, eta_min=1e-6)\n",
    "    # criterion = TaylorCrossEntropyLoss(n=2, smoothing=0.1)\n",
    "#     criterion = nn.BCEWithLogitsLoss()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    metric_fn = get_metric_fn\n",
    "\n",
    "    # Set system logger\n",
    "    system_logger = get_logger(name='train',file_path='train_log.log')\n",
    "\n",
    "    # Set trainer\n",
    "    scales = [\n",
    "        [140, 70], [180, 90], [220, 110], \n",
    "        [260, 130], [300, 150], [340, 170],\n",
    "        [380, 190], [420, 210], [460, 230]\n",
    "             ]\n",
    "\n",
    "    trainer = SAM_Trainer(criterion, model, device, metric_fn, optimizer, scheduler, logger=system_logger)\n",
    "    # trainer = MultiScale_Trainer(criterion, model, device, metric_fn, scales, optimizer, scheduler, logger=system_logger)\n",
    "\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "\n",
    "    valid_loss_history = []\n",
    "    valid_acc_history = []\n",
    "\n",
    "    best_val_loss = None\n",
    "    best_val_acc = None\n",
    "\n",
    "    \n",
    "    for epoch_index in tqdm(range(EPOCHS)):\n",
    "\n",
    "        trainer.train_epoch(train_dataloader, epoch_index)\n",
    "        trainer.validate_epoch(validation_dataloader, epoch_index)\n",
    "\n",
    "        train_loss_history.append(trainer.train_mean_loss)\n",
    "        train_acc_history.append(trainer.train_score)\n",
    "\n",
    "        valid_loss_history.append(trainer.val_mean_loss)\n",
    "        valid_acc_history.append(trainer.validation_score)\n",
    "\n",
    "        # don't use early stopper\n",
    "\n",
    "        if best_val_loss == None or trainer.val_mean_loss < best_val_loss:\n",
    "            best_val_loss = trainer.val_mean_loss\n",
    "            criterion = trainer.val_mean_loss\n",
    "            check_point = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'scheduler': scheduler.state_dict()\n",
    "            }\n",
    "            torch.save(check_point, os.path.join(WEIGHT_DIR, f'{BACKBONE}_best.pt'))\n",
    "\n",
    "    best_val_acc = max(valid_acc_history)\n",
    "\n",
    "    plot_loss(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    plot_acc(train_loss_history, valid_loss_history, BACKBONE)\n",
    "    \n",
    "    TRAINED_MODEL_PATH = os.path.join(WEIGHT_DIR, f'{BACKBONE}_best.pt')\n",
    "    \n",
    "    model.load_state_dict(torch.load(TRAINED_MODEL_PATH)['model'])\n",
    "\n",
    "    # Prediction\n",
    "    file_num_lst = []\n",
    "    pred_lst = []\n",
    "    prob_lst = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (img, file_num) in enumerate(test_dataloader):\n",
    "            img = img.to(device)\n",
    "            pred = F.softmax(model(img))\n",
    "            file_num_lst.extend(list(file_num))\n",
    "            prob_lst.extend(pred[:, 1].cpu().tolist())\n",
    "            pred_lst.extend(pred.argmax(dim=1).cpu().tolist())\n",
    "\n",
    "    df = pd.DataFrame({'file_name':list(map(int,file_num_lst)), 'answer':pred_lst, 'prob':prob_lst})\n",
    "    df.sort_values(by=['file_name'], inplace=True)\n",
    "    df.to_csv(os.path.join(RESULT_DIR, f'mask_pred_with_{BACKBONE}.csv'), index=False)\n",
    "\n",
    "    trainer.validate_epoch(check_dataloader, 0)\n",
    "    \n",
    "    # save data\n",
    "    \n",
    "    TIME = datetime.now().strftime('%m%d%H%M%S')\n",
    "\n",
    "    save_dict = {'backbone':BACKBONE,\n",
    "                 'time':TIME,\n",
    "                 'best_val_loss':best_val_loss,\n",
    "                 'best_val_acc':best_val_acc,\n",
    "                 'test_acc':None,\n",
    "                 'epochs':EPOCHS,\n",
    "                 'input_shape':INPUT_SHAPE,\n",
    "                 'augmentation':None,\n",
    "                 'optimizer':'Adam',\n",
    "                 'scheduler':'CosineAnnealingWarmRestarts',\n",
    "                 'attention_module':None,\n",
    "                 'learning_rate':LEARNING_RATE,\n",
    "                 'batch_size':BATCH_SIZE,\n",
    "                 'loss':'CrossEntropyLoss',\n",
    "                 'freeze':None,\n",
    "                 'others':None,\n",
    "                 'randomeseed':seed,\n",
    "                 'train_loss':train_loss_history,\n",
    "                 'train_acc':train_acc_history,\n",
    "                 'valid_loss':valid_loss_history,\n",
    "                 'valid_acc':valid_acc_history}\n",
    "\n",
    "\n",
    "    with open(f\"{os.path.join(NUMPY_DIR, BACKBONE)}_{TIME}.json\", 'w') as f:\n",
    "        json.dump(save_dict, f)\n",
    "    \n",
    "    wait_and_flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7beec4f4-4449-4ecd-a066-53468a356ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG_train_val_infer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2efef2-1f51-46f9-8998-244846297a52",
   "metadata": {},
   "source": [
    "# 기차 = Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "493e3052-9938-4e15-a885-3e2f91109ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_csv = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b13710a-5b6a-4b68-825d-1fc07e768490",
   "metadata": {},
   "source": [
    "# Efficient Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07ccf8c6-28e9-4e41-ae01-24e14330eb23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['tf_efficientnetv2_m_in21k']\n"
     ]
    }
   ],
   "source": [
    "print(len(EFF_BACKBONES))\n",
    "print(EFF_BACKBONES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8f6126c3-1c68-4080-8a28-202b53667da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6070207d-1cd1-4234-9686-2fb7562717c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 3.69 GiB already allocated; 12.00 MiB free; 3.96 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-479f1843907f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mEFF_train_val_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEFF_BACKBONES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mINDEX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-198b995232fc>\u001b[0m in \u001b[0;36mEFF_train_val_infer\u001b[0;34m(BACKBONE)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-9e79e96cf8de>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, dataloader, epoch_index)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-18bf80876d16>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_img)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_rate\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/efficientnet.py\u001b[0m in \u001b[0;36mforward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/timm/models/efficientnet_blocks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# Depth-wise convolution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_dw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 439\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 3.69 GiB already allocated; 12.00 MiB free; 3.96 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "EFF_train_val_infer(EFF_BACKBONES[INDEX])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568c01e1-55bf-49ff-939d-a842343dac4b",
   "metadata": {},
   "source": [
    "# NFNet Family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3e93011b-9d80-422a-9028-df54f4968af2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['eca_nfnet_l1', 'dm_nfnet_f1']\n"
     ]
    }
   ],
   "source": [
    "print(len(NFN_BACKBONES))\n",
    "print(NFN_BACKBONES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bbae3b6-55dd-4046-b7f6-d087a760ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e632a700-f9ba-4566-83f9-3d8b86af1fd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3437, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-45-a08759b6eb26>\", line 1, in <module>\n",
      "    NFN_train_val_infer(NFN_BACKBONES[INDEX])\n",
      "  File \"<ipython-input-36-7a98b0836af2>\", line 2, in NFN_train_val_infer\n",
      "    model = NFNMaskClassifier(BACKBONE).to(device)\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 852, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 530, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 552, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 850, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2061, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1515, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 1473, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 391, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 425, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/stephencha/anaconda3/envs/py38/lib/python3.8/posixpath.py\", line 167, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-45-a08759b6eb26>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNFN_train_val_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNFN_BACKBONES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mINDEX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-36-7a98b0836af2>\u001b[0m in \u001b[0;36mNFN_train_val_infer\u001b[0;34m(BACKBONE)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mNFN_train_val_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNFNMaskClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBACKBONE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2060\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2061\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2062\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2063\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2064\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/stephencha/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "NFN_train_val_infer(NFN_BACKBONES[INDEX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e7608b-e65c-46a3-8680-79e133bb70d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[48;5;230m\u001b[38;5;203m    ⸂⸂⸜(രᴗര๑)⸝⸃⸃    eca_nfnet_l1    ⸂⸂⸜(രᴗര๑)⸝⸃⸃    \u001b[0;0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 0.4061514938108408, Acc: 0.8146629489913072, ROC: 0.900396616392206\n",
      "Epoch 0, Val loss: 0.29354710052528327, Acc: 0.8693726937269373, ROC: 0.9521485904272661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [10:34<3:20:46, 634.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss: 0.23597602964943554, Acc: 0.9049532556995243, ROC: 0.969931671825834\n",
      "Epoch 1, Val loss: 0.2219913237281805, Acc: 0.9166051660516605, ROC: 0.9734185461619953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [21:03<3:09:27, 631.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train loss: 0.1766432789643234, Acc: 0.9370182056749221, ROC: 0.9837865834572588\n",
      "Epoch 2, Val loss: 0.1684007349275273, Acc: 0.944280442804428, ROC: 0.9852491823213101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [31:32<2:58:32, 630.16s/it]"
     ]
    }
   ],
   "source": [
    "INDEX = 1\n",
    "NFN_train_val_infer(NFN_BACKBONES[INDEX])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "01_mask_code.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "023f93d14c934299be94113aa089ff2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c01333330ae4863af15c0057fc3ae56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_71dbfe7632524d69b167b4d56d44ed41",
      "placeholder": "​",
      "style": "IPY_MODEL_ef0ccb555eb34b45a4c5af40649f68ce",
      "value": "100%"
     }
    },
    "2e1f1639316a49e4acccba120fb254d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2c01333330ae4863af15c0057fc3ae56",
       "IPY_MODEL_48d64e6e8d294ebd974fd9e610e935d0",
       "IPY_MODEL_bb54549aef364adca5e067690cf7258c"
      ],
      "layout": "IPY_MODEL_c972d82225b84aa3aa6cb513952162b9"
     }
    },
    "48d64e6e8d294ebd974fd9e610e935d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ce8f3ea6a7de46b08961e82f991b1a0e",
      "max": 21388428,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_023f93d14c934299be94113aa089ff2f",
      "value": 21388428
     }
    },
    "71dbfe7632524d69b167b4d56d44ed41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bb54549aef364adca5e067690cf7258c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bd2482a049e843f188adae6ee381101f",
      "placeholder": "​",
      "style": "IPY_MODEL_db323916e13f4d7da295d5ad05cf5c84",
      "value": " 20.4M/20.4M [00:00&lt;00:00, 71.6MB/s]"
     }
    },
    "bd2482a049e843f188adae6ee381101f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c972d82225b84aa3aa6cb513952162b9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ce8f3ea6a7de46b08961e82f991b1a0e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "db323916e13f4d7da295d5ad05cf5c84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ef0ccb555eb34b45a4c5af40649f68ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
